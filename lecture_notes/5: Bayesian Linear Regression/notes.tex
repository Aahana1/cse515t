\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage[scaled=0.8]{beramono}
\usepackage[margin=1.5in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{bm}

\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}

\usepackage{titlesec}
\titlespacing{\section}{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{5pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}

\usepackage{pgfplots}
\pgfplotsset{
  compat=newest,
  plot coordinates/math parser=false,
  tick label style={font=\footnotesize, /pgf/number format/fixed},
  label style={font=\small},
  legend style={font=\small},
  every axis/.append style={
    tick align=outside,
    clip mode=individual,
    scaled ticks=false,
    thick,
    tick style={semithick, black}
  }
}

\pgfkeys{/pgf/number format/.cd, set thousands separator={\,}}

\usepgfplotslibrary{external}
\tikzexternalize[prefix=tikz/]

\newlength\figurewidth
\newlength\figureheight

\setlength{\figurewidth}{8cm}
\setlength{\figureheight}{6cm}

\newlength\squarefigurewidth
\newlength\squarefigureheight

\setlength{\squarefigurewidth}{4cm}
\setlength{\squarefigureheight}{4cm}

\newlength\smallsquarefigurewidth
\newlength\smallsquarefigureheight

\setlength{\smallsquarefigurewidth}{3.25cm}
\setlength{\smallsquarefigureheight}{3.25cm}

\newlength\smallfigurewidth
\newlength\smallfigureheight

\setlength{\smallfigurewidth}{6.25cm}
\setlength{\smallfigureheight}{4cm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\given}{\mid}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\data}{\mc{D}}
\newcommand{\intd}[1]{\,\mathrm{d}{#1}}
\newcommand{\inv}{^{-1}}
\newcommand{\trans}{^\top}
\newcommand{\mat}[1]{\bm{\mathrm{#1}}}
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\epsilon}{\varepsilon}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\section*{Linear regression}

We are ready to consider our first machine-learning problem:
\emph{linear regression.}  Suppose that we are interested in the
values of a function $y(\vec{x})\colon \R^d \to \R$, where $\vec{x}$ is
a $d$-dimensional vector-valued input.  We assume that we have made
some observations of this mapping, $\data = \bigl\{ (\vec{x}_i, y_i)
\bigr\}_{i = 1}^N$ to serve as training data.  Given these examples,
the goal of regression is to be able to predict the value of $y$ at a
new input location $\vec{x}_\ast$.  In other words, we want to learn
from $\data$ a (hopefully accurate!) prediction function (also called
a \emph{regression function}) $\hat{y}(\vec{x}_\ast)$.

Without any further restrictions on $\hat{y}$, the problem is not
well-posed.  In general, I could choose any function $\hat{y}$.  How
do I choose a good function?  In particular, how do I effectively use
the training observations $\data$ to guide this choice?

The space of all possible regression functions $\hat{y}$ is enormous;
to restrict this space and make learning more tractable, \emph{linear
  regression} assumes the relationship between $\vec{x}$ and $y$ is
``mostly'' linear:
\begin{equation}
  \label{linear_assumption}
  y(\vec{x}) = \vec{x}\trans \vec{w} + \epsilon(\vec{x}),
\end{equation}
where $\vec{w} \in \R^d$ is a vector of parameters, and
$\epsilon{\vec{x}}$ represents a departure from the underlying linear
function $\vec{x}\trans \vec{w}$ at $\vec{x}$.  The value of
$\epsilon(\vec{x})$ is also called the \emph{residual.}  If the
underlying linear trend closely matches the data, we can expect
small residuals.

Note that this model does not explicitly contain an intercept term.
Instead, we normally add a constant feature to the input vector
$\vec{x}$ as in $\vec{x}' = [1, \vec{x}]\trans$.  In this case, the value
$w_1$ may be seen as an intercept term, allowing the hyperplane to
avoid passing through the origin at $\vec{x} = 0$.  To avoid messy
notation, we will not explicitly write the prime on the inputs and
rather assume this expansion has already been done.

Let us collect the $N$ training examples into a matrix of training
inputs $\vec{X} \in \R^{N \times d}$ and a vector of associated outputs
$\vec{y} \in \R^N$.  Then our assumed linear relationship in
\eqref{linear_assumption}, when applied to the training data, may be
written
\begin{equation*}
  \vec{y} = \mat{X}\vec{w} + \vec{\epsilon},
\end{equation*}
where we have also collected the residuals into the vector
$\vec{\epsilon}$.

There are many approaches for estimating the parameter vector
$\vec{w}$; below, we describe two.

\subsection*{Ordinary least squares}

As mentioned previously, a ``good'' linear mapping $\vec{x}\trans
\vec{w}$ should result in small residuals.  Our goal is to minimize
the magnitude of the residuals at every possible test input
$\vec{x}_\ast$.  Of course, this is impossible, so we settle for the
next best thing: we minimize the magnitude of the residuals on our
training data.

The classical approach is to estimate $\vec{w}$ by minimize the sum of
the squared residuals on the training data:
\begin{equation}
  \label{ols}
  \hat{\vec{w}}
  =
  \argmin_{\vec{w}}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2.
\end{equation}
This approach is called \emph{ordinary least squares.}  The underlying
assumption is that the magnitude of the residuals are all independent
from each other, so that we can simply sum up the squared residuals
and minimize.  (If the residuals were instead correlated, we would
have to consider the interaction between pairs of residuals.  This is
also possible and gives rise to \emph{generalized least squares.})

It turns out that the minimization problem in \eqref{ols} has an
exact, closed-form solution:
\begin{equation*}
  \hat{\vec{w}} = (\mat{X}\trans\mat{X})\inv \mat{X}\trans \vec{y}.
\end{equation*}

To predict the value of $y$ at a new input $\vec{x}_\ast$, we plug our
estimate of the parameter vector into \eqref{linear_assumption}:
\begin{equation*}
  \hat{y}(\vec{x}_\ast)
  =
  \vec{x}_\ast\trans \hat{\vec{w}} =
  \vec{x}_\ast\trans (\mat{X}\trans\mat{X})\inv \mat{X}\trans \vec{y}
  .
\end{equation*}

Ordinary least squares is by far the most commonly applied linear
regression procedure.

\subsection*{Ridge regression}

Occasionally, the ordinary least squares approach can lead to
problems.  In particular, when the training data lie extremely close
to a particular hyperplane (or when the number of observations is less
than the dimension, and we may find a hyperplane intersecting the data
exactly), the magnitudes of the estimated parameters $\hat{\vec{w}}$
can become ill-behaved and extremely large.  This is an example of
\emph{overfitting.}

\emph{A priori,} we might reasonably expect or desire the values of
$\vec{w}$ to be relatively small.  For this reason, so-called
\emph{penalization methods} or \emph{regularization methods} modify
the objective in \eqref{ols} to avoid such pathologies.  The most
common approach is to add a term to the objective encouraging small
entries of $\vec{w}$, for example
\begin{equation}
  \label{ridge}
  \hat{\vec{w}}
  =
  \argmin_{\vec{w}}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2
  +
  \lambda
  \lVert \vec{w} \rVert_{2}^2,
\end{equation}
where we have added the squared norm of $\vec{w}$ to the function to
be minimized, thereby penalizing large-magnitude entries of $\vec{w}$.
The scalar $\lambda \geq 0$ serves to trade off the contributions of
the residual term and the penalty term, and is a parameter of the
model that we may tune as desired.  As $\lambda \to 0$, we recover the
ordinary least squares objective.

The minimization problem in \eqref{ridge} may also be solved exactly,
giving the slightly modified estimator
\begin{equation*}
  \hat{\vec{w}} = (\mat{X}\trans\mat{X} + \lambda \mat{I})\inv \mat{X}\trans \vec{y}.
\end{equation*}
Again, as $\lambda \to 0$ (corresponding to very small penalties on
the magnitude of $\vec{w}$ entries), we see that the resulting
estimator converges to the ordinary least squares estimator.  As
$\lambda \to \infty$ (corresponding to very large penalties on the
magnitude of $\vec{w}$ entries), the estimator converges to $\vec{0}$.

This approach is known as \emph{ridge regression,} named for the
additional term $\lambda \mat{I}$ in the estimator, which when viewed
from ``above'' can whimsically be described as resembling a mountain
ridge.

\subsection*{Bayesian linear regression}

Here we consider a Bayesian approach to linear regression. Consider
again the assumed underlying linear relationship \eqref{linear_assumption}:
\begin{equation*}
  y(\vec{x}) = \vec{x}\trans \vec{w} + \epsilon(\vec{x}).
\end{equation*}
Here the vector $\vec{w}$ serves as the unknown parameter of the
model, which we will reason about using the Bayesian method.  For
convenience, we will select a multivariate Gaussian prior distribution
on $\vec{w}$:
\begin{equation*}
  p(\vec{w} \given \vec{\mu}, \mat{\Sigma})
  =
  \mc{N}(\vec{w}; \vec{mu}, \mat{\Sigma}).
\end{equation*}
As before, we might expect \emph{a priori} that the entries of
$\vec{w}$ are small.  Furthermore, in the absence of any evidence
otherwise, we might reason that all potential directions of $\vec{w}$
are equally likely.  These two assumptions can be codified by choosing
a multivariate Gaussian distribution for $\vec{w}$ with zero mean and
isotropic diagonal covariance $\tau^2 \mat{I}$:
\begin{equation*}
  p(\vec{w} \given \tau^2)
  =
  \mc{N}(\vec{w}; \vec{0}, \tau^2\mat{I}).
\end{equation*}

Given our observed data $\data = (\mat{X}, \vec{y})$, we wish to form
the posterior distribution $p(\vec{w} \given \data)$.  We will do so
by deriving the likelihood of our observed vector of values $\vec{y}$
given the parameter vector $\vec{w}$ and the inputs $\mat{X}$.  We
write again the assumed linear relationship in our training data:
\begin{equation*}
  \vec{y} = \mat{X}\vec{w} + \vec{\epsilon}.
\end{equation*}
Given $\vec{w}$ and $\mat{X}$, the only uncertainty in the above is
the additive residuals $\vec{\epsilon}$.  We assume that the residuals
are independent, unbiased, and tend to be ``small.''  We may
accomplish this by modeling each entry of $\vec{\epsilon}$ as arising
from zero-mean, independent, identically distributed Gaussian noise
with variance $\sigma^2$:
\begin{equation*}
  p(\vec{\epsilon} \given \sigma^2)
  =
  \mc{N}(\vec{\epsilon}; \vec{0}, \sigma^2 \mat{I}).
\end{equation*}
This assumption implies the following likelihood for the observations
$\vec{y}$:
\begin{equation*}
  p(\vec{y} \given \mat{X}, \vec{w}, \sigma^2)
  =
  \mc{N}(\vec{y}; \mat{X}\vec{w}, \sigma^2\mat{I}).
\end{equation*}

Bayes' theorem tells us that the posterior distribution $p(\vec{w}
\given \data)$ is proportional to the likelihood times the prior:
\begin{equation*}
  p(\vec{w} \given \mat{X}, \vec{y}, \sigma^2, \tau^2)
  \propto
  p(\vec{y} \given \mat{X}, \vec{w}, \sigma^2)
  p(\vec{w} \given \tau^2).
\end{equation*}

%% which we may rewrite as
%% \begin{equation*}
%%   \vec{\epsilon} = \vec{y} - \mat{X}\vec{w}.
%% \end{equation*}
%% Notice that $\vec{y} - \mat{X}\vec{w}$ is an affine transformation of
%% the Gaussian-distributed vector $\vec{w}$.  From our discussion of the
%% Gaussian distribution, this implies that the residuals have a
%% multivariate Gaussian distribution:
%% \begin{equation*}
%%   p(\vec{\epsilon} \given \vec{y}, \mat{X})
%%   =
%%   \mc{N}(\vec{\epsilon};
%%   \vec{y},
%%   \tau^2 \mat{X}\mat{X}\trans
%%   \vec{y} - \mat{X}\vec{w}.
%% \end{equation*}


\end{document}
