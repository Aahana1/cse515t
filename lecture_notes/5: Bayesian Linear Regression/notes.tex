\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage[scaled=0.8]{beramono}
\usepackage[margin=1.5in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{bm}

\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}

\usepackage{titlesec}
\titlespacing{\section}{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{5pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}

\usepackage{pgfplots}
\pgfplotsset{
  compat=newest,
  plot coordinates/math parser=false,
  tick label style={font=\footnotesize, /pgf/number format/fixed},
  label style={font=\small},
  legend style={font=\small},
  every axis/.append style={
    tick align=outside,
    clip mode=individual,
    scaled ticks=false,
    thick,
    tick style={semithick, black}
  }
}

\pgfkeys{/pgf/number format/.cd, set thousands separator={\,}}

\usepgfplotslibrary{external}
\tikzexternalize[prefix=tikz/]

\newlength\figurewidth
\newlength\figureheight

\setlength{\figurewidth}{8cm}
\setlength{\figureheight}{6cm}

\newlength\squarefigurewidth
\newlength\squarefigureheight

\setlength{\squarefigurewidth}{4cm}
\setlength{\squarefigureheight}{4cm}

\newlength\smallsquarefigurewidth
\newlength\smallsquarefigureheight

\setlength{\smallsquarefigurewidth}{3.25cm}
\setlength{\smallsquarefigureheight}{3.25cm}

\newlength\smallfigurewidth
\newlength\smallfigureheight

\setlength{\smallfigurewidth}{6.25cm}
\setlength{\smallfigureheight}{4cm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\given}{\mid}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\data}{\mc{D}}
\newcommand{\intd}[1]{\,\mathrm{d}{#1}}
\newcommand{\inv}{^{-1}}
\newcommand{\trans}{^\top}
\newcommand{\mat}[1]{\bm{\mathrm{#1}}}
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\epsilon}{\varepsilon}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\section*{Linear regression}

We are ready to consider our first machine-learning problem:
\emph{linear regression.}  Suppose that we are interested in the
values of a function $y(\vec{x})\colon \R^d \to \R$, where $\vec{x}$ is
a $d$-dimensional vector-valued input.  We assume that we have made
some observations of this mapping, $\data = \bigl\{ (\vec{x}_i, y_i)
\bigr\}_{i = 1}^N$, to serve as training data.  Given these examples,
the goal of regression is to be able to predict the value of $y$ at a
new input location $\vec{x}_\ast$.  In other words, we want to learn
from $\data$ a (hopefully accurate!) prediction function (also called
a \emph{regression function}) $\hat{y}(\vec{x}_\ast)$.

Without any further restrictions on $\hat{y}$, the problem is not
well-posed.  In general, I could choose any function $\hat{y}$.  How
do I choose a good function?  In particular, how do I effectively use
the training observations $\data$ to guide this choice?

The space of all possible regression functions $\hat{y}$ is enormous;
to restrict this space and make learning more tractable, \emph{linear
  regression} assumes the relationship between $\vec{x}$ and $y$ is
``mostly'' linear:
\begin{equation}
  \label{linear_assumption}
  y(\vec{x}) = \vec{x}\trans \vec{w} + \epsilon(\vec{x}),
\end{equation}
where $\vec{w} \in \R^d$ is a vector of parameters, and
$\epsilon(\vec{x})$ represents a departure from the underlying linear
function $\vec{x}\trans \vec{w}$ at $\vec{x}$.  The value of
$\epsilon(\vec{x})$ is also called the \emph{residual.}  If the
underlying linear trend closely matches the data, we can expect small
residuals.

Note that this model does not explicitly contain an intercept term.
Instead, we normally add a constant feature to the input vector
$\vec{x}$ as in $\vec{x}' = [1, \vec{x}]\trans$.  In this case, the value
$w_1$ may be seen as an intercept term, allowing the hyperplane to
avoid passing through the origin at $\vec{x} = 0$.  To avoid messy
notation, we will not explicitly write the prime on the inputs and
rather assume this expansion has already been done.

In general, we can imagine applying any function to $\vec{x}$ to serve
as the inputs to our linear regression model.  For example, to
accomplish polynomial regression of a given degree $k$, we might take
$\vec{x}' = [1, \vec{x}, \vec{x}^2, \dots, \vec{x}^k]\trans$, where
exponentiation is pointwise.  A transformation of this type may be
summarized by a map $\vec{x} \mapsto \phi(\vec{x})$; this is known as
a \emph{basis expansion.}  With a nonlinear basis expansion, we can
learn nonlinear regression functions.  We assume in the following that
such an expansion, if desired, has already been applied to the inputs.

Let us collect the $N$ training examples into a matrix of training
inputs $\vec{X} \in \R^{N \times d}$ and a vector of associated outputs
$\vec{y} \in \R^N$.  Then our assumed linear relationship in
\eqref{linear_assumption}, when applied to the training data, may be
written
\begin{equation*}
  \vec{y} = \mat{X}\vec{w} + \vec{\epsilon},
\end{equation*}
where we have also collected the residuals into the vector
$\vec{\epsilon}$.

There are many classical approaches for estimating the parameter
vector $\vec{w}$; below, we describe two.

\subsection*{Ordinary least squares}

As mentioned previously, a ``good'' linear mapping $\vec{x}\trans
\vec{w}$ should result in small residuals.  Our goal is to minimize
the magnitude of the residuals at every possible test input
$\vec{x}_\ast$.  Of course, this is impossible, so we settle for the
next best thing: we minimize the magnitude of the residuals on our
training data.

The classical approach is to estimate $\vec{w}$ by minimizing the sum of
the squared residuals on the training data:
\begin{equation}
  \label{ols}
  \hat{\vec{w}}
  =
  \argmin_{\vec{w}}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2.
\end{equation}
This approach is called \emph{ordinary least squares.}  The underlying
assumption is that the magnitude of the residuals are all independent
from each other, so that we can simply sum up the squared residuals
and minimize.  (If the residuals were instead correlated, we would
have to consider the interaction between pairs of residuals.  This is
also possible and gives rise to \emph{generalized least squares.})

It turns out that the minimization problem in \eqref{ols} has an
exact, closed-form solution:
\begin{equation*}
  \hat{\vec{w}} = (\mat{X}\trans\mat{X})\inv \mat{X}\trans \vec{y}.
\end{equation*}

To predict the value of $y$ at a new input $\vec{x}_\ast$, we plug our
estimate of the parameter vector into \eqref{linear_assumption}:
\begin{equation*}
  \hat{y}(\vec{x}_\ast)
  =
  \vec{x}_\ast\trans \hat{\vec{w}} =
  \vec{x}_\ast\trans (\mat{X}\trans\mat{X})\inv \mat{X}\trans \vec{y}
  .
\end{equation*}

Ordinary least squares is by far the most commonly applied linear
regression procedure.

\subsection*{Ridge regression}

Occasionally, the ordinary least squares approach can lead to
problems.  In particular, when the training data lie extremely close
to a particular hyperplane (or when the number of observations is less
than the dimension, and we may find a hyperplane intersecting the data
exactly), the magnitudes of the estimated parameters $\hat{\vec{w}}$
can become ill-behaved and extremely large.  This is an example of
\emph{overfitting.}

\emph{A priori,} we might reasonably expect or desire the values of
$\vec{w}$ to be relatively small.  For this reason, so-called
\emph{penalization} or \emph{regularization methods} modify the
objective in \eqref{ols} to avoid such pathologies.  The most common
approach is to add a term to the objective encouraging small entries
of $\vec{w}$, for example
\begin{equation}
  \label{ridge}
  \hat{\vec{w}}
  =
  \argmin_{\vec{w}}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2
  +
  \lambda
  \lVert \vec{w} \rVert_{2}^2,
\end{equation}
where we have added the squared 2-norm of $\vec{w}$ to the function to
be minimized, thereby penalizing large-magnitude entries of $\vec{w}$.
The scalar $\lambda \geq 0$ serves to trade off the contributions of
the residual term and the penalty term, and is a parameter of the
model that we may tune as desired.  As $\lambda \to 0$, we recover the
ordinary least squares objective.

The minimization problem in \eqref{ridge} may also be solved exactly,
giving the slightly modified estimator
\begin{equation*}
  \hat{\vec{w}}
  =
  (\mat{X}\trans\mat{X} + \lambda \mat{I})\inv \mat{X}\trans \vec{y}.
\end{equation*}
Again, as $\lambda \to 0$ (corresponding to very small penalties on
the magnitude of $\vec{w}$ entries), we see that the resulting
estimator converges to the ordinary least squares estimator.  As
$\lambda \to \infty$ (corresponding to very large penalties on the
magnitude of $\vec{w}$ entries), the estimator converges to $\vec{0}$.

This approach is known as \emph{ridge regression,} named for the
additional term $\lambda \mat{I}$ in the estimator, which when viewed
from ``above'' can whimsically be described as resembling a mountain
ridge.

\subsection*{Bayesian linear regression}

Here we consider a Bayesian approach to linear regression. Consider
again the assumed underlying linear relationship \eqref{linear_assumption}:
\begin{equation*}
  y(\vec{x}) = \vec{x}\trans \vec{w} + \epsilon(\vec{x}).
\end{equation*}
Here the vector $\vec{w}$ serves as the unknown parameter of the
model, which we will reason about using the Bayesian method.  For
convenience, we will select a multivariate Gaussian prior distribution
on $\vec{w}$:
\begin{equation*}
  p(\vec{w} \given \vec{\mu}, \mat{\Sigma})
  =
  \mc{N}(\vec{w}; \vec{\mu}, \mat{\Sigma}).
\end{equation*}
As before, we might expect \emph{a priori} that the entries of
$\vec{w}$ are small.  Furthermore, in the absence of any evidence
otherwise, we might reason that all potential directions of $\vec{w}$
are equally likely.  These two assumptions can be codified by choosing
a multivariate Gaussian distribution for $\vec{w}$ with zero mean and
isotropic diagonal covariance $s^2 \mat{I}$:
\begin{equation}
  \label{simple_prior}
  p(\vec{w} \given s^2)
  =
  \mc{N}(\vec{w}; \vec{0}, s^2\mat{I}).
\end{equation}

Given our observed data $\data = (\mat{X}, \vec{y})$, we wish to form
the posterior distribution $p(\vec{w} \given \data)$.  We will do so
by deriving the joint distribution between the weight vector $\vec{w}$
and the observed vector of values $\vec{y}$, given the inputs
$\mat{X}$.  We write again the assumed linear relationship in our
training data:
\begin{equation*}
  \vec{y} = \mat{X}\vec{w} + \vec{\epsilon}.
\end{equation*}
Given $\vec{w}$ and $\mat{X}$, the only uncertainty in the above is
the additive residuals $\vec{\epsilon}$.  We assume that the residuals
are independent, unbiased, and tend to be ``small.''  We may
accomplish this by modeling each entry of $\vec{\epsilon}$ as arising
from zero-mean, independent, identically distributed Gaussian noise
with variance $\sigma^2$:
\begin{equation*}
  p(\vec{\epsilon} \given \sigma^2)
  =
  \mc{N}(\vec{\epsilon}; \vec{0}, \sigma^2 \mat{I}).
\end{equation*}

Define $\vec{f} = \mat{X}\vec{w},$ and note that $\vec{f}$ is a linear
transformation of the multivariate-Gaussian distributed $\vec{w}$;
therefore, we may easily derive its distribution:
\begin{equation*}
  p(\vec{f} \given \mat{X}, \vec{\mu}, \mat{\Sigma})
  =
  \mc{N}(\vec{f}; \mat{X}\vec{\mu}, \mat{X}\mat{\Sigma}\mat{X}\trans).
\end{equation*}
Now the observation vector $\vec{y}$ is a sum of two independent
multivariate Gaussian distributed vectors $\vec{f}$ and
$\vec{\epsilon}$.  Therefore, we may derive its prior distribution as
well:
\begin{equation*}
  p(\vec{y} \given \vec{X}, \vec{\mu}, \mat{\Sigma}, \sigma^2)
  =
  \mc{N}(
  \vec{y};
  \mat{X}\vec{\mu},
  \mat{X}\mat{\Sigma}\mat{X}\trans + \sigma^2\mat{I}
  ).
\end{equation*}
Finally, we compute the covariance between $\vec{y}$ and $\vec{w}$:
\begin{equation*}
  \cov[\vec{y}, \vec{w}]
  =
  \cov[\mat{X}\vec{w} + \vec{\epsilon}, \vec{w}]
  =
  \cov[\mat{X}\vec{w}, \vec{w}]
  =
  \mat{X}\cov[\vec{w}, \vec{w}]
  =
  \mat{X}\mat{\Sigma},
\end{equation*}
where we have used the linearity of covariance and the fact that
the noise vector $\vec{\epsilon}$ is independent of $\vec{w}$.

Now the joint distribution of $\vec{w}$ and $\vec{y}$ is multivariate
Gaussian:
\begin{equation*}
  p\Biggl(
  \begin{bmatrix}
    \vec{w}
    \\
    \vec{y}
  \end{bmatrix}
  \given
  \vec{X}, \vec{\mu}, \mat{\Sigma}, \sigma^2
  \Biggr)
  =
  \mc{N}
  \Biggl(
  \begin{bmatrix}
    \vec{w}
    \\
    \vec{y}
  \end{bmatrix}
  ;
  \begin{bmatrix}
    \vec{\mu}
    \\
    \mat{X}\vec{\mu}
  \end{bmatrix}
  ,
  \begin{bmatrix}
    \mat{\Sigma} & \mat{\Sigma}\mat{X}\trans
    \\
    \mat{X}\mat{\Sigma} & \mat{X}\mat{\Sigma}\mat{X}\trans + \sigma^2\mat{I}
  \end{bmatrix}
  \Biggr).
\end{equation*}
We apply the formula for conditioning multivariate Gaussians on
subvectors to derive the posterior distribution of $\vec{w}$ given
the data $\data$:
\begin{equation*}
  p(\vec{w} \given \data, \vec{\mu}, \mat{\Sigma}, \sigma^2)
  =
  \mc{N}(\vec{w};
  \vec{\mu}_{\vec{w}\given\data},
  \mat{\Sigma}_{\vec{w}\given\data}
  ),
\end{equation*}
where
\begin{align*}
  \vec{\mu}_{\vec{w}\given\data}
  &=
  \vec{\mu}
  +
  \mat{\Sigma}
  \mat{X}\trans
  (\mat{X}\mat{\Sigma}\mat{X}\trans + \sigma^2 \mat{I})\inv
  (\vec{y} - \mat{X}\vec{w});
  \\
  \mat{\Sigma}_{\vec{w}\given\data}
  &=
  \mat{\Sigma}
  -
  \mat{\Sigma}
  \mat{X}\trans
  (\mat{X}\mat{\Sigma}\mat{X}\trans + \sigma^2 \mat{I})\inv
  \mat{X}
  \mat{\Sigma}.
\end{align*}

\subsection*{Making predictions}

Suppose we wish to use our model to predict the outputs $\vec{y}_\ast$
associated with a set of inputs $\mat{X}_\ast$.  Writing again
$\vec{y}^\ast = \mat{X}_\ast\vec{w} + \vec{\epsilon}$, we derive:
\begin{equation*}
  p(\vec{y}_\ast \given \data, \vec{\mu}, \mat{\Sigma}, \sigma^2)
  =
  \mc{N}(
  \vec{y}_\ast;
  \mat{X}_\ast \vec{\mu}_{\vec{w}\given\data},
  \mat{X}_\ast \mat{\Sigma}_{\vec{w}\given\data} \mat{X}_\ast\trans + \sigma^2 \mat{I}).
\end{equation*}
Therefore, we have a full, joint probability distribution over the
outputs $\vec{y}_\ast$.  If we must make point estimates, we choose a
loss function for our predictions and use Bayesian decision theory.
For example, to minimize expected squared loss, we predict the
posterior mean.

An equivalent way to derive this result is to use the sum rule as
follows:
\begin{align*}
  p(\vec{y}_\ast \given \data, \vec{\mu}, \mat{\Sigma}, \sigma^2)
  &=
  \int
  p(\vec{y}_\ast \given \vec{w}, \sigma^2)
  p(\vec{w} \given \data, \vec{\mu}, \mat{\Sigma}, \sigma^2)
  \intd{\vec{w}}
  \\
  &=
  \int
  \mc{N}(\vec{y}_\ast; \mat{X}_\ast \vec{w}, \sigma^2 \mat{I})
  \,
  \mc{N}(\vec{w}; \vec{\mu}_{\vec{w}\given\data}, \mat{\Sigma}_{\vec{w}\given\data})
  \intd{\vec{w}}
  \\
  &=
  \mc{N}(
  \vec{y}_\ast;
  \mat{X}_\ast \vec{\mu}_{\vec{w}\given\data},
  \mat{X}_\ast \mat{\Sigma}_{\vec{w}\given\data} \mat{X}_\ast\trans + \sigma^2 \mat{I}),
\end{align*}
where we have used a slightly more general form of the convolution
formula for multivariate Gaussians.  In this case, we view the
prediciton process as considering the predictions we would make with
every possible value of the parameters $\vec{w}$ and averaging them
according to their plausibility.

\subsection*{Connection to ridge regression}

For the simple prior \eqref{simple_prior}, the posterior mean
can be rewritten as:
\begin{equation*}
  \vec{\mu}_{\vec{w}\given\data}
  =
  s^2
  \mat{X}\trans
  (s^2\mat{X}\mat{X}\trans + \sigma^2 \mat{I})\inv
  \vec{y}
  =
  \biggl(\mat{X}\trans\mat{X} + \frac{\sigma^2}{s^2} \mat{I}\biggr)\inv
  \mat{X}\trans
  \vec{y},
\end{equation*}
where we have factored out the $s^2$ in the inverse and have applied
the matrix identity
\begin{equation*}
  (\mat{A}\mat{B} + c\mat{I})\inv \mat{A} =
  \mat{A}(\mat{B}\mat{A} + c \mat{I})\inv.
\end{equation*}
This is exactly equal to the ridge regression solution \eqref{ridge}
with $\lambda = \frac{\sigma^2}{s^2}$!  Why is this?

Consider finding the \emph{maximum a posteriori} estimator of
$\vec{w}$ using the simple prior \eqref{simple_prior}.
Bayes' theorem tells us that the posterior distribution $p(\vec{w}
\given \data)$ is proportional to the likelihood times the prior:
\begin{equation*}
  p(\vec{w} \given \mat{X}, \vec{y}, \sigma^2, s^2)
  \propto
  p(\vec{y} \given \mat{X}, \vec{w}, \sigma^2)
  p(\vec{w} \given s^2).
\end{equation*}
The distribution of $\vec{y}$ given $\vec{w}$ is simple:
\begin{equation*}
  p(\vec{y} \given \mat{X}, \vec{w}, \sigma^2)
  =
  \mc{N}(\vec{y}; \mat{X}\vec{w}, \sigma^2\mat{I})
  =
  \prod_{i = 1}^N
  \mc{N}(y_i; \vec{x}_i\trans\vec{w}, \sigma^2).
\end{equation*}

Rather than maximizing the posterior directly, we may instead maximize
its logarithm:
\begin{align*}
  \hat{\vec{w}}_{\text{\acro{MAP}}}
  &=
  \argmax_{\vec{w}}
  -\frac{1}{2\sigma^2}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2
  -
  \frac{1}{2s^2}
  \sum_{i = 1}^d
  w_i^2
  \\
  &=
  \argmin_{\vec{w}}
  \sum_{i = 1}^N
  (\vec{x}_i\trans \vec{w} - y_i)^2
  +
  \frac{\sigma^2}{s^2}
  \lVert \vec{w} \rVert_{2}^{2},
\end{align*}
where we multiplied by $-2\sigma^2$ to derive the second line.  This
is exactly the ridge regression objective.  Therefore ridge regression
with $\ell^2$ regularization can be seen from the Bayesian perspective
as placing a Gaussian prior on $\vec{w}$ and finding the \acro{MAP}
estimator.  In general, many regularization methods can be interpreted
as implicitly choosing a particular likelihood for the data and prior
for the parameters of a model and maximizing the posterior density.
For example, $\ell^1$ regularization (as seen in \acro{LASSO}) is
equivalent to placing a Laplace distribution prior on the weight
parameters, encouraging a sparse solution.

Continuing with this argument, we can interpret the squared loss
function $\sum_{i=1}^N (\vec{x}\trans \vec{w} - y_i)^2$ as coming from
an iid Gaussian noise assumption.  In general, loss functions in
minimization problems such as \eqref{ridge} can often be interpreted
as negative log likelihoods arising from an implicit independent noise
assumption.

\end{document}
