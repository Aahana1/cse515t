\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage[scaled=0.8]{beramono}
\usepackage[margin=1.5in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{bm}

\usepackage{amsthm}
\newtheorem{defn}{Definition}

\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}

\usepackage{titlesec}
\titlespacing{\section}{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{5pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}

\usepackage{pgfplots}
\pgfplotsset{
  compat=newest,
  plot coordinates/math parser=false,
  tick label style={font=\footnotesize, /pgf/number format/fixed},
  label style={font=\small},
  legend style={font=\small},
  every axis/.append style={
    tick align=outside,
    clip mode=individual,
    scaled ticks=false,
    thick,
    tick style={semithick, black}
  }
}

\pgfkeys{/pgf/number format/.cd, set thousands separator={\,}}

\usepgfplotslibrary{external}
\tikzexternalize[prefix=tikz/]

\newlength\figurewidth
\newlength\figureheight

\setlength{\figurewidth}{12cm}
\setlength{\figureheight}{6cm}

\newlength\squarefigurewidth
\newlength\squarefigureheight

\setlength{\squarefigurewidth}{4cm}
\setlength{\squarefigureheight}{4cm}

\newlength\smallsquarefigurewidth
\newlength\smallsquarefigureheight

\setlength{\smallsquarefigurewidth}{3.25cm}
\setlength{\smallsquarefigureheight}{3.25cm}

\newlength\smallfigurewidth
\newlength\smallfigureheight

\setlength{\smallfigurewidth}{6.25cm}
\setlength{\smallfigureheight}{4cm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\given}{\mid}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\data}{\mc{D}}
\newcommand{\intd}[1]{\,\mathrm{d}{#1}}
\newcommand{\inv}{^{-1}}
\newcommand{\trans}{^\top}
\newcommand{\mat}[1]{\bm{\mathrm{#1}}}
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Exp}{\mathbb{E}}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\section*{Gaussian Processes}

We previously considered the \emph{kernel trick,} which allowed
us to define the \emph{kernel function}
\begin{equation*}
  K(\vec{x}, \vec{x}')
  =
  \phi(\vec{x})\trans\mat{\Sigma}\phi(\vec{x}').
\end{equation*}
Then, when doing Bayesian linear regression with prior
\begin{equation*}
  p(\vec{w}) = \mc{N}(\vec{w}; \vec{0}, \mat{\Sigma}),
\end{equation*}
and data $\data$, we derived the predictive distribution
\begin{equation*}
  p(\vec{y}_\ast \given \mat{X}_\ast, \data, \sigma^2)
  =
  \mc{N}(
  \vec{y}_\ast;
  \vec{\mu}_{\vec{y}_\ast \given \data},
  \mat{K}_{\vec{y}_\ast \given \data}),
\end{equation*}
where
\begin{align*}
  \vec{\mu}_{\vec{y}_\ast \given \data}
  &=
  \mat{K}_\ast\trans
  (\mat{K} + \sigma^2\mat{I})\inv
  \vec{y}
  \\
  \mat{K}_{\vec{y}_\ast \given \data}
  &=
  \mat{K}_{\ast\ast}
  -
  \mat{K}_\ast\trans
  (\mat{K} + \sigma^2\mat{I})\inv
  \mat{K}_\ast,
\end{align*}
and we have defined
\begin{equation*}
  \mat{K} = K(\mat{X}, \mat{X})
  \qquad
  \mat{K}_\ast = K(\mat{X}, \mat{X}_\ast)
  \qquad
  \mat{K}_{\ast\ast} = K(\mat{X}_\ast, \mat{X}_\ast).
\end{equation*}

Examining the form of this predictive distribution, we barely need to
perform inference over the weight vector $\vec{w}$ at all; its
covariance simply appears in the definition of $K$.  If we treat $K$
as a black-box, maybe we can skip the inference over $\vec{w}$
entirely and simply work with $K$ directly.  This line of thinking
motivates \emph{Gaussian processes.}

Let us again write down the abstract problem of regression.  We have
an unknown function $f\colon \mc{X} \to \R$, where $\mc{X}$ is an
arbitrary input space (for example $\mc{X} = \R^d$).  Given noisy
training observations $\data = (\mat{X}, \vec{y})$, we wish to infer
the value of $f$ at a set of test points $\mat{X}_\ast$.

Here we will continue to assume that the values $\vec{y}$ are
generated by adding zero-mean, independent Gaussian noise with
variance $\sigma^2$ to the true function values $\vec{f} =
f(\mat{X})$.  Probabilistically, we write
\begin{equation*}
  p(\vec{y} \given \vec{f}, \sigma^2)
  =
  \mc{N}(\vec{y}; \vec{f}, \sigma^2 \mat{I}).
\end{equation*}
In linear regression, we made the assumption that $f$ was linear:
$f(\vec{x}) = \phi(\vec{x})\trans\vec{w}$.  In this case we would have
$\vec{f} = \mat{\Phi}\vec{w}$, and a trivial rewriting of this
likelihood would reveal it to be the same as we have been using all
along, except that we are removing any explicit assumptions about the
nature of the function $f$.

When discussing Bayesian linear regression, we also derived the prior
distribution for the vector of latent values $\vec{f} =
\mat{\Phi}\vec{w}$ given the input locations $\mat{X}$.  In
particular, given the prior $p(\vec{w}) = \mc{N}(\vec{w}; \vec{0},
\mat{\Sigma})$, we have
\begin{equation*}
  p(\vec{f} \given \mat{X})
  =
  \mc{N}(\vec{f}; \vec{0}, \mat{\Phi}\mat{\Sigma}\mat{\Phi}\trans)
  =
  \mc{N}(\vec{f}; \vec{0}, \mat{K}),
\end{equation*}
where we have used the definition of the kernel function above.
Again, if we treat $K$ as a black-box, we have avoided explicit
reasoning about $\vec{w}$ or $\phi$.

\emph{Gaussian processes} approach the problem of regression using
these ideas in a somewhat more abstract approach.  We construct an
explicit prior distribution for the \emph{function} $f$, that we
reason about via the Bayesian method.  This seems like it might be
difficult, because $f$ might in general be infinite-dimensional.  A
Gaussian process provides a natural extension of the familiar
multivariate Gaussian distribution to potentially infinite-dimensional
function spaces.  The key definition is the following.
\begin{defn}
  A \emph{Gaussian process} is a (potentially infinite) collection of
  random variables such that the joint distribution of any finite
  number of them is multivariate Gaussian.
\end{defn}
So the way we extend the finite-dimensional Gaussian distribution is
by allowing arbitrary inputs but only ever reasoning about finitely
many of them at a time.

A Gaussian process prior distribution on $f$ is written
\begin{equation*}
  p(f) = \mc{GP}(f; \mu, K),
\end{equation*}
and just like the multivariate Gaussian distribution, is
parameterized by its first two moments (now functions):
\begin{itemize}
\item $\Exp[f] = \mu\colon \mc{X} \to \R$, the \emph{mean function,} and
\item $\Exp\bigl[\bigl(f(x) - \mu(x)\bigr)\bigl(f(x') -
  \mu(x')\bigr)\bigr] = K\colon \mc{X} \times \mc{X} \to \R$, a
  positive semidefinite \emph{covariance function} or
  \emph{kernel.}\footnote{A function is \emph{positive semidefinite}
    if, for every finite set of points $\mat{X}$, the \emph{Gram
      matrix} $\mat{K} = K(\mat{X}, \mat{X})$ is positive
    semidefinite.  Note that a matrix is a valid covariance matrix if
    and only if it is positive semidefinite.}
\end{itemize}
The function $K$ should look familiar!  Rather than explicitly
construct $K$, we simply select any positive semidefinite function,
any one of which will define a Gaussian process.

The mean function encodes the central tendency of the function, and is
often assumed to be a constant (usually zero).  The covariance
function encodes information about the shape and structure we expect
the function to have.  A simple and very common example is the
\emph{squared exponential} covariance:
\begin{equation*}
  K(\vec{x}, \vec{x}'; \lambda, \ell)
  =
  \lambda^2
  \exp\biggl(-\frac{\lVert \vec{x} - \vec{x}' \rVert^2}{2\ell^2}\biggr),
\end{equation*}
which encodes the notation that ``nearby points should have similar
function values.''

Suppose we have selected a GP prior $\mc{GP}(f; \mu, K)$ for the
function $f$.  Consider a finite set of points $\mat{X} \subseteq
\mc{X}$.  The GP prior on $f$, by definition, implies the following
joint distribution on the associated function values $\vec{f} =
f(\mat{X})$:
\begin{equation*}
  p(\vec{f} \given \mat{X})
  =
  \mc{N}(\vec{f}; \mu(\mat{X}), K(\mat{X}, \mat{X})\bigr).
\end{equation*}
That is, we simply evaluate the mean and covariance functions at
$\mat{X}$ and take the associated multivariate Gaussian distribution.
Very simple!  Notice the similarity to the expression above, except
that we have regained the ability to use non-zero prior means via the
(arbitrary) prior mean function $\mu$.


\end{document}
